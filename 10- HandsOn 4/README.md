# RNN Hands-On 4

This assignment's objectives are to train English to German Seq2Seq machine translation model using attention as well as techniques such as packed padded sequences and masking. This helps in the performance of the model as well as improving training time. Following model variants are covered-

1. Attention based Seq2Seq machine translation 

2. Attention based Seq2Seq machine translation with packed padded sequences and masking. Packed padded sequences are used to tell RNN to skip over padding tokens in encoder. Masking explicitly forces the model to ignore certain values, such as attention over padded elements. This helps in model performance as well as reduce training time.
